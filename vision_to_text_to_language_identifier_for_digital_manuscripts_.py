# -*- coding: utf-8 -*-
"""Vision to Text to Language Identifier for Digital Manuscripts .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fhY0G1o0-QH1VIFF9yc86VTy65mlaE9I

Importing Neccessary labriaries
"""

!sudo apt-get update
!sudo apt-get install -y tesseract-ocr
!pip install pytesseract Pillow langdetect opencv-python

from google.colab import files

uploaded = files.upload()

for filename in uploaded.keys():
    print(f'User uploaded file "{filename}" with length {len(uploaded[filename])} bytes')

"""Transfering Mar.traineddata to the tessdata directory"""

import shutil

# Move the uploaded file to the tessdata directory
shutil.move('mar.traineddata', '/usr/share/tesseract-ocr/4.00/tessdata/mar.traineddata')

# Verify the file has been moved
!ls /usr/share/tesseract-ocr/4.00/tessdata/

"""Importing OS and then Verifying the environment variable to make sure the environment is setuped"""

import os

# Set the TESSDATA_PREFIX environment variable
os.environ['TESSDATA_PREFIX'] = '/usr/share/tesseract-ocr/4.00/tessdata/'

# Verify the environment variable
print("TESSDATA_PREFIX:", os.environ['TESSDATA_PREFIX'])

"""Checking which languages are availble to work on"""

# List available languages
!tesseract --list-langs

"""Pre-Processing the images to train the model and then extracting the text after OCR, then identifying the language and showing the extracted text"""

from PIL import Image
import cv2
import numpy as np
import pytesseract
from langdetect import detect

def preprocess_image(image_path):
    image = Image.open(image_path)
    gray_image = image.convert('L')
    np_image = np.array(gray_image)
    _, bin_image = cv2.threshold(np_image, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    return Image.fromarray(bin_image)

def extract_text_from_image(image):
    text = pytesseract.image_to_string(image, lang='mar')
    return text

def identify_language(text):
    language = detect(text)
    return language

# Loop through uploaded images and process them
image_dir = '/content/sample_data'

for image_name in os.listdir(image_dir):
    if image_name.endswith(('.jpg', '.jpeg', '.png')):
        image_path = os.path.join(image_dir, image_name)
        preprocessed_image = preprocess_image(image_path)
        extracted_text = extract_text_from_image(preprocessed_image)
        identified_language = identify_language(extracted_text)
        print(f"Image: {image_name}")
        print(f"Extracted Text: {extracted_text}")
        print(f"Detected Language: {identified_language}")
        print("="*50)

"""Now to translate the extracted text to make it understandble"""

!pip install googletrans==4.0.0-rc1

"""Loading the extracted text to google translation model and translating the text to English for better understanding"""

!sudo apt-get update
!sudo apt-get install -y tesseract-ocr
!pip install pytesseract Pillow langdetect opencv-python googletrans==4.0.0-rc1

from PIL import Image
import cv2
import numpy as np
import pytesseract
from langdetect import detect
from googletrans import Translator

# Set the TESSDATA_PREFIX environment variable
import os
os.environ['TESSDATA_PREFIX'] = '/usr/share/tesseract-ocr/4.00/tessdata/'

def preprocess_image(image_path):
    image = Image.open(image_path)
    gray_image = image.convert('L')
    np_image = np.array(gray_image)
    _, bin_image = cv2.threshold(np_image, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    return Image.fromarray(bin_image)

def extract_text_from_image(image):
    text = pytesseract.image_to_string(image, lang='mar')
    return text

def identify_language(text):
    language = detect(text)
    return language

def translate_text(text, dest_language='en'):
    translator = Translator()
    translation = translator.translate(text, dest=dest_language)
    return translation.text

# Loop through uploaded images and process them
image_dir = '/content/sample_data'

for image_name in os.listdir(image_dir):
    if image_name.endswith(('.jpg', '.jpeg', '.png')):
        image_path = os.path.join(image_dir, image_name)
        preprocessed_image = preprocess_image(image_path)
        extracted_text = extract_text_from_image(preprocessed_image)
        identified_language = identify_language(extracted_text)
        translated_text = translate_text(extracted_text, dest_language='en')
        print(f"Image: {image_name}")
        print(f"Extracted Text: {extracted_text}")
        print(f"Detected Language: {identified_language}")
        print(f"Translated Text: {translated_text}")
        print("="*50)

"""Conclusion:

      The translated text is not completly correct and much of meaning have been loss,
      while translating but even the extracted text from images where not interpreted exactly as orignal.
      The images where of old text maninly containg Modi script(old marathi language derived from directly Old Sanskrit)
      due to which half of text extraction was not accurate
"""